{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794b9fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved as 'ngram_model_report.docx'\n"
     ]
    }
   ],
   "source": [
    "# First install python-docx if you haven't already:\n",
    "# pip install python-docx\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt, RGBColor\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.enum.style import WD_STYLE_TYPE\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "def create_ngram_report():\n",
    "    # Create a new Document\n",
    "    doc = Document()\n",
    "    \n",
    "    # Set document margins\n",
    "    sections = doc.sections\n",
    "    for section in sections:\n",
    "        section.top_margin = Inches(1)\n",
    "        section.bottom_margin = Inches(1)\n",
    "        section.left_margin = Inches(1)\n",
    "        section.right_margin = Inches(1)\n",
    "    \n",
    "    # Title\n",
    "    title = doc.add_heading('N-Gram Language Model Assignment Report', 0)\n",
    "    title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    \n",
    "    # Executive Summary\n",
    "    doc.add_heading('Executive Summary', level=1)\n",
    "    doc.add_paragraph(\n",
    "        'This report analyzes the implementation and performance of N-gram language models trained on a corpus of 6.2 million tokens from multiple sources. Our experiments demonstrate that trigram models (n=3) achieve optimal performance with a perplexity of 36.20, representing a 57.4% improvement over bigram models. The implementation successfully addresses key challenges including out-of-vocabulary words through smoothing techniques and computational efficiency through optimized data structures.'\n",
    "    )\n",
    "    \n",
    "    # 1. Introduction\n",
    "    doc.add_heading('1. Introduction', level=1)\n",
    "    doc.add_paragraph(\n",
    "        'N-gram models serve as fundamental building blocks in natural language processing, providing probabilistic predictions of word sequences based on local context. This project implements and evaluates N-gram models of varying orders (2-5) on a diverse corpus combining Reuters, Brown, Gutenberg, and web text sources.'\n",
    "    )\n",
    "    \n",
    "    # 2. Methodology\n",
    "    doc.add_heading('2. Methodology', level=1)\n",
    "    \n",
    "    doc.add_heading('2.1 Data Collection and Preprocessing', level=2)\n",
    "    doc.add_paragraph(\n",
    "        'The experimental setup utilized a comprehensive corpus drawn from four distinct sources within the NLTK package, ensuring diverse linguistic patterns and domains. The data underwent systematic preprocessing to enhance model quality:'\n",
    "    )\n",
    "    \n",
    "    p = doc.add_paragraph()\n",
    "    p.add_run('Corpus Statistics:').bold = True\n",
    "    \n",
    "    # Add bullet points for statistics\n",
    "    doc.add_paragraph('Total tokens: 6,196,290', style='List Bullet')\n",
    "    doc.add_paragraph('Training set: 4,957,032 tokens (80%)', style='List Bullet')\n",
    "    doc.add_paragraph('Testing set: 1,239,258 tokens (20%)', style='List Bullet')\n",
    "    doc.add_paragraph('Vocabulary size: 106,731 unique tokens', style='List Bullet')\n",
    "    \n",
    "    doc.add_paragraph(\n",
    "        'The preprocessing pipeline removed punctuation marks, numerical values, and other non-linguistic noise to create a cleaner token stream. This standardization process proved crucial for reducing sparsity and improving model generalization.'\n",
    "    )\n",
    "    \n",
    "    doc.add_heading('2.2 Model Architecture', level=2)\n",
    "    doc.add_paragraph(\n",
    "        'The N-gram model implementation centers on an efficient dictionary-based architecture for storing context-word frequency pairs. This design choice offers several advantages:'\n",
    "    )\n",
    "    \n",
    "    doc.add_paragraph('Constant-time lookups: O(1) average case complexity for frequency retrieval', style='List Bullet')\n",
    "    doc.add_paragraph('Memory efficiency: Direct mapping from (context, word) tuples to frequencies', style='List Bullet')\n",
    "    doc.add_paragraph('Scalability: Handles the 106,731-word vocabulary without performance degradation', style='List Bullet')\n",
    "    \n",
    "    doc.add_paragraph('The probability calculation follows the maximum likelihood estimation approach:')\n",
    "    \n",
    "    # Add formula as a styled paragraph\n",
    "    formula = doc.add_paragraph()\n",
    "    formula.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    formula_run = formula.add_run('P(w_i | w_{i-n+1}, ..., w_{i-1}) = count(w_{i-n+1}, ..., w_i) / count(w_{i-n+1}, ..., w_{i-1})')\n",
    "    formula_run.font.name = 'Courier New'\n",
    "    formula_run.font.size = Pt(10)\n",
    "    \n",
    "    doc.add_heading('2.3 Smoothing Techniques', level=2)\n",
    "    doc.add_paragraph(\n",
    "        'To address the zero-probability problem for unseen N-grams, three smoothing methods were implemented:'\n",
    "    )\n",
    "    \n",
    "    # Numbered list\n",
    "    doc.add_paragraph('Laplace Smoothing: Adds a constant α to all counts, ensuring non-zero probabilities', style='List Number')\n",
    "    doc.add_paragraph('Stupid Backoff: Recursively backs off to lower-order N-grams with a penalty factor', style='List Number')\n",
    "    doc.add_paragraph('Kneser-Ney Smoothing: Employs sophisticated discounting based on continuation probabilities', style='List Number')\n",
    "    \n",
    "    doc.add_paragraph(\n",
    "        'The adjustable hyperparameter α allows fine-tuning the smoothing strength based on corpus characteristics.'\n",
    "    )\n",
    "    \n",
    "    # 3. Results and Analysis\n",
    "    doc.add_heading('3. Results and Analysis', level=1)\n",
    "    \n",
    "    doc.add_heading('3.1 Perplexity Evaluation', level=2)\n",
    "    doc.add_paragraph(\n",
    "        'Perplexity measurements across different N-gram orders reveal compelling insights:'\n",
    "    )\n",
    "    \n",
    "    # Create table\n",
    "    table = doc.add_table(rows=5, cols=3)\n",
    "    table.style = 'Light Grid Accent 1'\n",
    "    \n",
    "    # Add header row\n",
    "    header_cells = table.rows[0].cells\n",
    "    header_cells[0].text = 'N-gram Order'\n",
    "    header_cells[1].text = 'Perplexity'\n",
    "    header_cells[2].text = 'Relative Change'\n",
    "    \n",
    "    # Add data rows\n",
    "    data = [\n",
    "        ['2 (Bigram)', '85.09', 'Baseline'],\n",
    "        ['3 (Trigram)', '36.20', '-57.4%'],\n",
    "        ['4 (4-gram)', '36.54', '+0.9%'],\n",
    "        ['5 (5-gram)', '40.55', '+11.0%']\n",
    "    ]\n",
    "    \n",
    "    for i, row_data in enumerate(data):\n",
    "        row_cells = table.rows[i+1].cells\n",
    "        for j, cell_data in enumerate(row_data):\n",
    "            row_cells[j].text = cell_data\n",
    "            # Bold the trigram row\n",
    "            if i == 1:\n",
    "                for paragraph in row_cells[j].paragraphs:\n",
    "                    for run in paragraph.runs:\n",
    "                        run.bold = True\n",
    "    \n",
    "    doc.add_paragraph(\n",
    "        'The trigram model emerges as the optimal configuration, achieving the lowest perplexity of 36.20. This represents a dramatic 57.4% improvement over bigram models, indicating that two-word contexts capture significant linguistic dependencies.'\n",
    "    )\n",
    "    \n",
    "    doc.add_heading('3.2 Performance Analysis', level=2)\n",
    "    doc.add_paragraph('The results demonstrate a clear pattern:')\n",
    "    \n",
    "    # Add bullet points with bold keywords\n",
    "    p1 = doc.add_paragraph('', style='List Bullet')\n",
    "    p1.add_run('Sharp improvement').bold = True\n",
    "    p1.add_run(' from bigram to trigram models suggests that two-word contexts provide substantial predictive power')\n",
    "    \n",
    "    p2 = doc.add_paragraph('', style='List Bullet')\n",
    "    p2.add_run('Diminishing returns').bold = True\n",
    "    p2.add_run(' beyond trigrams indicate that longer contexts introduce sparsity without proportional gains')\n",
    "    \n",
    "    p3 = doc.add_paragraph('', style='List Bullet')\n",
    "    p3.add_run('Performance degradation').bold = True\n",
    "    p3.add_run(' at n=5 likely stems from overfitting to training data patterns')\n",
    "    \n",
    "    doc.add_heading('3.3 Text Generation Quality', level=2)\n",
    "    doc.add_paragraph(\n",
    "        'The sentence generation module, which iteratively predicts words based on context, produced coherent outputs for trigram models. The generation process maintains linguistic fluency by:'\n",
    "    )\n",
    "    \n",
    "    doc.add_paragraph('Starting with high-probability seed contexts', style='List Bullet')\n",
    "    doc.add_paragraph('Applying smoothing to handle novel word combinations', style='List Bullet')\n",
    "    doc.add_paragraph('Terminating at specified lengths or natural boundaries', style='List Bullet')\n",
    "    \n",
    "    # 4. Challenges and Solutions\n",
    "    doc.add_heading('4. Challenges and Solutions', level=1)\n",
    "    \n",
    "    doc.add_heading('4.1 Computational Complexity', level=2)\n",
    "    p = doc.add_paragraph()\n",
    "    p.add_run('Challenge:').bold = True\n",
    "    p.add_run(' Training time increases exponentially as n decreases, particularly problematic for unigram and bigram models processing 6M+ tokens.')\n",
    "    \n",
    "    p = doc.add_paragraph()\n",
    "    p.add_run('Solution:').bold = True\n",
    "    p.add_run(' Implemented optimization strategies including:')\n",
    "    \n",
    "    doc.add_paragraph('Batch processing of N-gram extraction', style='List Bullet')\n",
    "    doc.add_paragraph('Efficient string manipulation using optimized Python libraries', style='List Bullet')\n",
    "    doc.add_paragraph('Pre-computation of frequently accessed statistics', style='List Bullet')\n",
    "    \n",
    "    doc.add_heading('4.2 Memory Management', level=2)\n",
    "    p = doc.add_paragraph()\n",
    "    p.add_run('Challenge:').bold = True\n",
    "    p.add_run(' Storing frequency counts for all possible N-grams with a 106,731-word vocabulary threatens memory exhaustion.')\n",
    "    \n",
    "    p = doc.add_paragraph()\n",
    "    p.add_run('Solution:').bold = True\n",
    "    \n",
    "    doc.add_paragraph('Sparse representation storing only observed N-grams', style='List Bullet')\n",
    "    doc.add_paragraph('Dynamic pruning of low-frequency entries', style='List Bullet')\n",
    "    doc.add_paragraph('Efficient dictionary implementation with hash-based lookups', style='List Bullet')\n",
    "    \n",
    "    doc.add_heading('4.3 Data Sparsity', level=2)\n",
    "    p = doc.add_paragraph()\n",
    "    p.add_run('Challenge:').bold = True\n",
    "    p.add_run(' Higher-order models suffer from severe sparsity, with many valid word sequences appearing zero times in training data.')\n",
    "    \n",
    "    p = doc.add_paragraph()\n",
    "    p.add_run('Solution:').bold = True\n",
    "    p.add_run(' The three-pronged smoothing approach ensures robust probability estimates even for unseen sequences, with Kneser-Ney smoothing providing particularly effective handling of rare events.')\n",
    "    \n",
    "    # 5. Conclusions and Future Work\n",
    "    doc.add_heading('5. Conclusions and Future Work', level=1)\n",
    "    doc.add_paragraph(\n",
    "        'This implementation successfully demonstrates the practical application of N-gram language models on a substantial corpus. Key findings include:'\n",
    "    )\n",
    "    \n",
    "    doc.add_paragraph('Optimal Model Order: Trigram models provide the best balance between contextual information and data sparsity', style='List Number')\n",
    "    doc.add_paragraph('Smoothing Importance: Proper smoothing techniques are essential for handling the long tail of language', style='List Number')\n",
    "    doc.add_paragraph('Efficiency Considerations: Careful implementation choices enable processing of multi-million token corpora', style='List Number')\n",
    "    doc.add_paragraph('Dataset Scale: Larger datasets can significantly improve prediction accuracy by providing more comprehensive coverage of language patterns and reducing the impact of data sparsity', style='List Number')\n",
    "    \n",
    "    doc.add_heading('Future Directions', level=2)\n",
    "    doc.add_paragraph('Several avenues for improvement merit exploration:')\n",
    "    \n",
    "    future_items = [\n",
    "        ('Larger Dataset Integration:', ' Expanding the training corpus beyond 6M tokens to capture more diverse linguistic patterns and improve model generalization'),\n",
    "        ('Adaptive N-gram Selection:', ' Dynamically choosing N based on available context'),\n",
    "        ('Class-based Models:', ' Reducing sparsity through word clustering'),\n",
    "        ('Neural Augmentation:', ' Combining N-gram statistics with neural embeddings'),\n",
    "        ('Domain Adaptation:', ' Investigating performance across specific text genres'),\n",
    "        ('Parallel Processing:', ' Implementing distributed training for handling massive datasets efficiently')\n",
    "    ]\n",
    "    \n",
    "    for title, desc in future_items:\n",
    "        p = doc.add_paragraph('', style='List Bullet')\n",
    "        p.add_run(title).bold = True\n",
    "        p.add_run(desc)\n",
    "    \n",
    "    doc.add_paragraph(\n",
    "        'The success of this implementation validates N-gram models as robust baselines for language modeling tasks, while highlighting areas where modern neural approaches might provide advantages. The achieved perplexity of 36.20 for trigram models represents strong performance given the model\\'s simplicity and interpretability.'\n",
    "    )\n",
    "    \n",
    "    # References\n",
    "    doc.add_heading('References', level=1)\n",
    "    \n",
    "    references = [\n",
    "        'Chen, S. F., & Goodman, J. (1996). An empirical study of smoothing techniques for language modeling. Proceedings of the 34th Annual Meeting of the ACL.',\n",
    "        'Kneser, R., & Ney, H. (1995). Improved backing-off for M-gram language modeling. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing.',\n",
    "        'Manning, C. D., & Schütze, H. (1999). Foundations of Statistical Natural Language Processing. MIT Press.'\n",
    "    ]\n",
    "    \n",
    "    for i, ref in enumerate(references, 1):\n",
    "        doc.add_paragraph(f'{i}. {ref}', style='List Number')\n",
    "    \n",
    "    # Save the document\n",
    "    doc.save('ngram_model_report.docx')\n",
    "    print(\"Report saved as 'ngram_model_report.docx'\")\n",
    "\n",
    "# Run the function to create the document\n",
    "if __name__ == \"__main__\":\n",
    "    create_ngram_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2787f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
