{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have the required dependencies, uncomment and run the following lines:\n",
    "# !pip install nltk numpy\n",
    "\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Tuple, Dict, Set, Optional\n",
    "from nltk.corpus import reuters, brown, gutenberg, webtext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f0d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb52d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    text = ''.join([char for char in text if not char.isdigit()])\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_training_data() -> str:\n",
    "    logging.getLogger(\"nltk\").setLevel(logging.ERROR)\n",
    "    nltk.download('reuters')\n",
    "    nltk.download('brown')\n",
    "    nltk.download('gutenberg')\n",
    "    nltk.download('webtext')\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "    print(\"Gathering texts from multiple corpora...\")\n",
    "    \n",
    "    reuters_text = ' '.join(reuters.raw(file_id) for file_id in reuters.fileids())\n",
    "    brown_text = ' '.join(brown.raw(file_id) for file_id in brown.fileids())\n",
    "    gutenberg_text = ' '.join(gutenberg.raw(file_id) for file_id in gutenberg.fileids())\n",
    "    webtext_text = ' '.join(webtext.raw(file_id) for file_id in webtext.fileids())\n",
    "    \n",
    "    combined_text = (\n",
    "        reuters_text +\n",
    "        ' ' + brown_text +\n",
    "        ' ' + reuters_text +\n",
    "        ' ' + webtext_text +\n",
    "        ' ' + brown_text +\n",
    "        ' ' + reuters_text\n",
    "    )\n",
    "\n",
    "    preprocessed_text = preprocess_text(combined_text)\n",
    "    \n",
    "    token_count = len(preprocessed_text.split())\n",
    "    print(f\"Total tokens in combined corpus: {token_count}\")\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def get_training_and_test_data(test_ratio=0.1, random_seed=42):\n",
    "    import random\n",
    "    random.seed(random_seed)\n",
    "    combined_text = get_training_data()\n",
    "    all_tokens = combined_text.split()\n",
    "    vocabulary = set(all_tokens)\n",
    "    vocab_size = len(vocabulary)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    random.shuffle(all_tokens)\n",
    "    split_idx = int(len(all_tokens) * (1 - test_ratio))\n",
    "    train_tokens = all_tokens[:split_idx]\n",
    "    test_tokens = all_tokens[split_idx:]\n",
    "    print(f\"Training set: {len(train_tokens)} tokens\")\n",
    "    print(f\"Test set: {len(test_tokens)} tokens\")\n",
    "    return train_tokens, test_tokens, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd327a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def create_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "def count_ngrams(ngram_list: List[Tuple[str, ...]]) -> Dict[Tuple[str, ...], int]:\n",
    "    return Counter(ngram_list)\n",
    "\n",
    "def get_vocabulary(tokens: List[str]) -> Set[str]:\n",
    "    return set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f13c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.b Write a function to calculate the probability of a word following a given (n − 1)- gram\n",
    "def calculate_probability(\n",
    "    word: str,\n",
    "    context: Tuple[str, ...], #context here is the n-1 gram.\n",
    "    ngram_counts: Dict[Tuple[str, ...], int],\n",
    "    context_counts: Dict[Tuple[str, ...], int],\n",
    "    vocabulary_size: int,\n",
    "    smoothing: str = 'laplace'\n",
    ") -> float:\n",
    "    \n",
    "\n",
    "    ngram = context + (word,)\n",
    "    \n",
    "    # 2.e Implement smoothing techniques (like Laplace smoothing) to handle the issues of zero probabilities for unseen n-grams.\n",
    "    if smoothing == 'none':\n",
    "        # No smoothing - just return relative frequency\n",
    "        if context in context_counts and context_counts[context] > 0:\n",
    "            return ngram_counts.get(ngram, 0) / context_counts[context]\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    elif smoothing == 'laplace':\n",
    "        # Laplace (add-1) smoothing\n",
    "        numerator = ngram_counts.get(ngram, 0) + 1\n",
    "        denominator = context_counts.get(context, 0) + vocabulary_size\n",
    "        return numerator / denominator\n",
    "\n",
    "    elif smoothing == 'add_k':\n",
    "        # Add-k smoothing\n",
    "        numerator = ngram_counts.get(ngram, 0) + alpha\n",
    "        denominator = context_counts.get(context, 0) + alpha * vocabulary_size\n",
    "        return numerator / denominator\n",
    "\n",
    "    elif smoothing == 'katz':\n",
    "        # Simple Katz backoff (discounting not implemented, just backoff)\n",
    "        if ngram_counts.get(ngram, 0) > 0 and context_counts.get(context, 0) > 0:\n",
    "            return ngram_counts[ngram] / context_counts[context]\n",
    "        else:\n",
    "            # Backoff to lower order (unigram)\n",
    "            return 1 / vocabulary_size\n",
    "\n",
    "    elif smoothing == 'kneser_ney':\n",
    "        # Simple absolute discounting Kneser-Ney (D=0.75)\n",
    "        D = 0.75\n",
    "        count = ngram_counts.get(ngram, 0)\n",
    "        context_count = context_counts.get(context, 0)\n",
    "        if context_count > 0:\n",
    "            discounted = max(count - D, 0) / context_count\n",
    "            # Continuation probability: number of unique contexts this word follows / total number of contexts\n",
    "            continuation_count = sum(1 for ng in ngram_counts if ng[-1] == word)\n",
    "            total_contexts = len(context_counts)\n",
    "            lambda_weight = (D / context_count) * len([ng for ng in ngram_counts if ng[:-1] == context])\n",
    "            p_continuation = continuation_count / total_contexts if total_contexts > 0 else 1 / vocabulary_size\n",
    "            return discounted + lambda_weight * p_continuation\n",
    "        else:\n",
    "            # If context unseen, use uniform probability\n",
    "            return 1 / vocabulary_size\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown smoothing method: {smoothing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f0b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.c Write a function to predict the next word given a sequence of words based on these probabilities.\n",
    "def predict_next_word(\n",
    "    context: Tuple[str, ...],\n",
    "    vocabulary: Set[str],\n",
    "    ngram_counts: Dict[Tuple[str, ...], int],\n",
    "    context_counts: Dict[Tuple[str, ...], int],\n",
    "    n: int,\n",
    "    smoothing: str = 'laplace',\n",
    "    k: int = 5\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Predict the next word given a context.\n",
    "    \n",
    "    Args:\n",
    "        context: Tuple of preceding words\n",
    "        vocabulary: Set of all possible words\n",
    "        ngram_counts: Dictionary of n-gram counts\n",
    "        context_counts: Dictionary of (n-1)-gram counts\n",
    "        n: The n in n-gram\n",
    "        smoothing: Smoothing technique\n",
    "        k: Number of top predictions to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (word, probability) tuples for the k most likely next words\n",
    "    \"\"\"\n",
    "\n",
    "    if len(context) < n - 1:\n",
    "        context = ('the',) * ((n - 1) - len(context)) + tuple(context)\n",
    "    else:\n",
    "        context = tuple(context[-(n-1):])\n",
    "    \n",
    "    vocabulary_size = len(vocabulary)\n",
    "    word_probs = []\n",
    "    \n",
    "    for word in vocabulary:\n",
    "        prob = calculate_probability(\n",
    "            word, context, ngram_counts, context_counts, \n",
    "            vocabulary_size, smoothing)\n",
    "        word_probs.append((word, prob))\n",
    "    return sorted(word_probs, key=lambda x: x[1], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d412bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word_main( vocabulary, ngram_counts, context_counts,n):\n",
    "    context = (\"I\", \"would\")\n",
    "    smoothing = 'laplace'\n",
    "    print(f\"Context: {context}\")\n",
    "    print(\"Next words suggestions:\")\n",
    "\n",
    "    for word, prob in predict_next_word(context, vocabulary, ngram_counts, context_counts,n, smoothing, k=5):\n",
    "        print(f\"  {word}: {prob:.6f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774981e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.d Write a function to generate a sentence of a specified length given a prefix of (n−1)(or less) words\n",
    "\n",
    "def generate_sentence(\n",
    "    length: int,\n",
    "    vocabulary: Set[str],\n",
    "    ngram_counts: Dict[Tuple[str, ...], int],\n",
    "    context_counts: Dict[Tuple[str, ...], int],\n",
    "    n: int,\n",
    "    prefix: Optional[List[str]] = None,\n",
    "    smoothing: str = 'laplace'\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate a sentence of specified length given an optional prefix.\n",
    "    \n",
    "    Args:\n",
    "        length: Target length of the sentence in words\n",
    "        vocabulary: Set of all possible words\n",
    "        ngram_counts: Dictionary of n-gram counts\n",
    "        context_counts: Dictionary of (n-1)-gram counts\n",
    "        n: The n in n-gram\n",
    "        prefix: Optional list of starting words\n",
    "        smoothing: Smoothing technique\n",
    "        alpha: Smoothing parameter for add-k smoothing\n",
    "        \n",
    "    Returns:\n",
    "        Generated sentence as a list of words\n",
    "    \"\"\"\n",
    "    if prefix is None:\n",
    "        prefix = random.choice(list(context_counts.keys()))\n",
    "    else:\n",
    "        if len(prefix) < n - 1:\n",
    "            prefix = ('the',) * ((n - 1) - len(prefix)) + tuple(prefix)\n",
    "        else:\n",
    "            prefix = tuple(prefix[-(n-1):])\n",
    "    \n",
    "    sentence = list(prefix)\n",
    "    \n",
    "    for _ in range(length - len(prefix)):\n",
    "        context = tuple(sentence[-(n-1):])\n",
    "        next_word_candidates = predict_next_word(\n",
    "            context, vocabulary, ngram_counts, context_counts, \n",
    "            n, smoothing\n",
    "        )\n",
    "        if not next_word_candidates or next_word_candidates[0][1] == 0:\n",
    "            next_word = random.choice(list(vocabulary))\n",
    "        else:\n",
    "            words, probs = zip(*next_word_candidates)\n",
    "            probs = np.array(probs)\n",
    "            probs = probs / probs.sum()\n",
    "            next_word = np.random.choice(words, p=probs)\n",
    "        sentence.append(next_word)\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a310017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_generation(\n",
    "    vocabulary: Set[str],\n",
    "    ngram_counts: Dict[Tuple[str, ...], int],\n",
    "    context_counts: Dict[Tuple[str, ...], int],\n",
    "    n: int,\n",
    "    test_prefixes: List[List[str]],\n",
    "    num_words: int = 10,\n",
    "    smoothing: str = 'laplace'\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Test the model by generating text from various prefixes.\n",
    "    \n",
    "    Args:\n",
    "        vocabulary: Set of all words in the vocabulary\n",
    "        ngram_counts: Dictionary of n-gram counts\n",
    "        context_counts: Dictionary of (n-1)-gram counts\n",
    "        n: Size of n-grams\n",
    "        test_prefixes: List of prefix lists to test (each prefix should have at most n-1 words)\n",
    "        num_words: Number of words to generate after each prefix\n",
    "        smoothing: Smoothing technique ('none', 'laplace', or 'good_turing')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping prefix strings to generated text lists\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for prefix in test_prefixes:\n",
    "        # Ensure prefix is not longer than n-1 words\n",
    "        if len(prefix) >= n:\n",
    "            prefix = prefix[-(n-1):]\n",
    "            \n",
    "        # Generate text using the prefix\n",
    "        generated_text = generate_sentence(\n",
    "            length=num_words,\n",
    "            vocabulary=vocabulary,\n",
    "            ngram_counts=ngram_counts,\n",
    "            context_counts=context_counts,\n",
    "            n=n,\n",
    "            prefix=prefix,\n",
    "            smoothing=smoothing\n",
    "        )\n",
    "        \n",
    "        # Store result with prefix as key\n",
    "        prefix_str = ' '.join(prefix)\n",
    "        results[prefix_str] = generated_text\n",
    "        \n",
    "        # Print the generated text\n",
    "        print(f\"\\nPrefix: '{prefix_str}'\")\n",
    "        print(f\"Generated: '{' '.join(generated_text)}'\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd95dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(test_tokens, ngram_counts, context_counts, vocabulary_size, n, smoothing='laplace'):\n",
    "\n",
    "    test_ngrams = create_ngrams(test_tokens, n)\n",
    "    total_log_prob = 0.0\n",
    "\n",
    "    unseen_ngrams = 0\n",
    "    unseen_contexts = 0\n",
    "\n",
    "    for ngram in test_ngrams:\n",
    "        context = ngram[:-1]\n",
    "        ngram_count = ngram_counts.get(ngram, 0)\n",
    "        context_count = context_counts.get(context, 0)\n",
    "        if ngram_count == 0:\n",
    "            unseen_ngrams += 1\n",
    "        if context_count == 0:\n",
    "            unseen_contexts += 1\n",
    "        if smoothing == 'none':\n",
    "            if context_count > 0 and ngram_count > 0:\n",
    "                prob = ngram_count / context_count\n",
    "            else:\n",
    "                prob = 1 / vocabulary_size\n",
    "        elif smoothing == 'laplace':\n",
    "            prob = (ngram_count + alpha) / (context_count + alpha * vocabulary_size)\n",
    "        else:\n",
    "            prob = (ngram_count + alpha) / (context_count + alpha * vocabulary_size)\n",
    "        prob = max(prob, 1e-10)\n",
    "        total_log_prob += math.log2(prob)\n",
    "\n",
    "    avg_log_prob = total_log_prob / len(test_ngrams)\n",
    "    perplexity = 2 ** (-avg_log_prob)\n",
    "\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c909468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_ngram_models(\n",
    "    train_tokens: List[str],\n",
    "    test_tokens: List[str],\n",
    "    n_values: List[int],\n",
    "    smoothing: str = 'laplace'\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Compare performance of n-gram models with different n values.\n",
    "    \n",
    "    Args:\n",
    "        train_tokens: List of tokens from training set\n",
    "        test_tokens: List of tokens from test set\n",
    "        n_values: List of n values to test\n",
    "        smoothing: Smoothing technique ('none', 'laplace', or 'good_turing')\n",
    "        alpha: Smoothing parameter for Laplace smoothing\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping n values to perplexity scores\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    vocabulary = get_vocabulary(train_tokens)\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    print(f\"\\nComparing n-gram models:\")\n",
    "    print(f\"{'n':<5}{'Perplexity':<15}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    for n in n_values:\n",
    "        # Create and count n-grams for the training set\n",
    "        ngram_list = create_ngrams(train_tokens, n)\n",
    "        ngram_counts = count_ngrams(ngram_list)\n",
    "        \n",
    "        # Create and count (n-1)-grams for context\n",
    "        context_list = create_ngrams(train_tokens, n-1)\n",
    "        context_counts = count_ngrams(context_list)\n",
    "        \n",
    "        # Calculate perplexity on test set\n",
    "        perplexity = calculate_perplexity(\n",
    "            test_tokens=test_tokens,\n",
    "            ngram_counts=ngram_counts,\n",
    "            context_counts=context_counts,\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            n=n,\n",
    "            smoothing=smoothing\n",
    "        )\n",
    "        \n",
    "        results[n] = perplexity\n",
    "        print(f\"{n:<5}{perplexity:<15.4f}\")\n",
    "    \n",
    "    # Find the best model (lowest perplexity)\n",
    "    best_n = min(results.items(), key=lambda x: x[1])[0]\n",
    "    print(f\"\\nBest model: n={best_n} with perplexity={results[best_n]:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44733730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(tokens: List[str], train_ratio: float = 0.8) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Split tokens into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of all tokens\n",
    "        train_ratio: Ratio of tokens to use for training\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (training_tokens, testing_tokens)\n",
    "    \"\"\"\n",
    "    split_idx = int(len(tokens) * train_ratio)\n",
    "    return tokens[:split_idx], tokens[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cfdb0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nGram_generation(n, train_tokens, vocabulary, smoothing='laplace'):\n",
    "    # Test the model with various prefixes for different n-gram sizes\n",
    "    print(\"\\n--- Testing Text Generation with Various Prefixes for Different n-gram Sizes ---\")\n",
    "    ngram_prefixes = {\n",
    "        2: [\n",
    "            [\"the\"],\n",
    "            [\"in\"],\n",
    "            [\"it\"],\n",
    "            [\"they\"],\n",
    "            [\"she\"]\n",
    "        ],\n",
    "        3: [\n",
    "            [\"the\", \"market\"],\n",
    "            [\"in\", \"the\"],\n",
    "            [\"it\", \"is\"],\n",
    "            [\"they\", \"were\"],\n",
    "            [\"she\", \"was\"]\n",
    "        ],\n",
    "        4: [\n",
    "            [\"the\", \"stock\", \"market\"],\n",
    "            [\"in\", \"the\", \"city\"],\n",
    "            [\"it\", \"is\", \"a\"],\n",
    "            [\"they\", \"were\", \"not\"],\n",
    "            [\"she\", \"was\", \"very\"]\n",
    "        ],\n",
    "        5: [\n",
    "            [\"the\", \"stock\", \"market\", \"is\"],\n",
    "            [\"in\", \"the\", \"middle\", \"of\"],\n",
    "            [\"it\", \"is\", \"a\", \"good\"],\n",
    "            [\"they\", \"were\", \"not\", \"able\"],\n",
    "            [\"she\", \"was\", \"very\", \"happy\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def test_ngram_generation(n, train_tokens, vocabulary, test_prefixes, num_words=8, smoothing='laplace'):\n",
    "        ngram_list = create_ngrams(train_tokens, n)\n",
    "        ngram_counts = count_ngrams(ngram_list)\n",
    "        context_list = create_ngrams(train_tokens, n-1)\n",
    "        context_counts = count_ngrams(context_list)\n",
    "        print(f\"\\n--- {n}-gram Model Text Generation ---\")\n",
    "        for prefix in test_prefixes:\n",
    "            sentence = generate_sentence(\n",
    "                length=num_words,\n",
    "                vocabulary=vocabulary,\n",
    "                ngram_counts=ngram_counts,\n",
    "                context_counts=context_counts,\n",
    "                n=n,\n",
    "                prefix=prefix,\n",
    "                smoothing=smoothing\n",
    "            )\n",
    "            print(f\"Prefix {prefix}: {' '.join(sentence)}\")\n",
    "\n",
    "    for n in range(2, 6):\n",
    "        test_ngram_generation(\n",
    "            n=n,\n",
    "            train_tokens=train_tokens,\n",
    "            vocabulary=vocabulary,\n",
    "            test_prefixes=ngram_prefixes[n],\n",
    "            num_words=8,\n",
    "            smoothing='laplace'\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a9a2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_language_model(tokens, n):\n",
    "    \"\"\"Analyze the n-gram model statistics for diagnostic purposes\"\"\"\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"Total tokens: {len(tokens)}\")\n",
    "    print(f\"Unique tokens (vocabulary size): {len(set(tokens))}\")\n",
    "    \n",
    "    # Create and count n-grams and contexts\n",
    "    ngrams = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "    contexts = list(zip(*[tokens[i:] for i in range(n-1)]))\n",
    "    \n",
    "    ngram_counts = Counter(ngrams)\n",
    "    context_counts = Counter(contexts)\n",
    "    \n",
    "    print(f\"Total {n}-grams: {len(ngrams)}\")\n",
    "    print(f\"Unique {n}-grams: {len(ngram_counts)}\")\n",
    "    print(f\"Unique contexts ({n-1}-grams): {len(context_counts)}\")\n",
    "    \n",
    "    # Analyze n-gram counts\n",
    "    counts = list(ngram_counts.values())\n",
    "    print(f\"Min n-gram count: {min(counts)}\")\n",
    "    print(f\"Max n-gram count: {max(counts)}\")\n",
    "    print(f\"Mean n-gram count: {np.mean(counts):.2f}\")\n",
    "    print(f\"N-grams appearing once: {counts.count(1)} ({counts.count(1)/len(counts)*100:.2f}%)\")\n",
    "    \n",
    "    # Check a few probability calculations\n",
    "    print(\"\\nSample probability calculations:\")\n",
    "    for i, ngram in enumerate(list(ngram_counts.keys())[:3]):\n",
    "        context = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        \n",
    "        ngram_count = ngram_counts[ngram]\n",
    "        context_count = context_counts[context]\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        mle_prob = ngram_count / context_count\n",
    "        laplace_prob_a1 = (ngram_count + 1) / (context_count + len(set(tokens)))\n",
    "        laplace_prob_a01 = (ngram_count + 0.1) / (context_count + 0.1 * len(set(tokens)))\n",
    "        \n",
    "        print(f\"N-gram {i+1}: {ngram}\")\n",
    "        print(f\"  Count: {ngram_count}, Context count: {context_count}\")\n",
    "        print(f\"  MLE probability: {mle_prob:.10f}\")\n",
    "        print(f\"  Laplace (α=1) probability: {laplace_prob_a1:.10f}\")\n",
    "        print(f\"  Laplace (α=0.1) probability: {laplace_prob_a01:.10f}\")\n",
    "    \n",
    "    return ngram_counts, context_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ca2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perplexity_for_ngram_range(vocabulary: Set[str], test_tokens: List[str], train_tokens: List[str]):\n",
    "    \"\"\"\n",
    "    Calculate and print perplexity for n-gram models with n from 2 to 5.\n",
    "    \"\"\"\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    for n in range(2, 6):\n",
    "        ngram_list = create_ngrams(train_tokens, n)\n",
    "        ngram_counts = count_ngrams(ngram_list)\n",
    "        context_list = create_ngrams(train_tokens, n-1)\n",
    "        context_counts = count_ngrams(context_list)\n",
    "        perplexity = calculate_perplexity(\n",
    "            test_tokens=test_tokens,\n",
    "            ngram_counts=ngram_counts,\n",
    "            context_counts=context_counts,\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            n=n,\n",
    "            smoothing='laplace'\n",
    "        )\n",
    "        print(f\"Perplexity for n={n}: {perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c28005e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering texts from multiple corpora...\n",
      "Total tokens in combined corpus: 6195257\n",
      "\n",
      "Sample of cleaned text: asian exporters fear damage from usjapan rift mounting trade friction between the us and japan has raised fears among many of asias exporting nations that the row could inflict farreaching economic damage businessmen and officials said they told reuter correspondents in asian capitals a us move against japan might boost protectionist sentiment in the us and lead to curbs on american imports of their products but some exporters said that while the conflict would hurt them in the longrun in the sh\n",
      "Total tokens: 6196290\n",
      "Training tokens: 4957032\n",
      "Testing tokens: 1239258\n",
      "Vocabulary size: 106731\n",
      "\n",
      " 2.A. Sucessfully created n-grams from the tokenized text and calculated their frequencies in the dataset.\n",
      "\n",
      " 2.B\n",
      "Probability of 'market' following 'the': 0.004563\n",
      "\n",
      " 2.C\n",
      "Context: ('I', 'would')\n",
      "Next words suggestions:\n",
      "  be: 0.020260\n",
      "  not: 0.006319\n",
      "  have: 0.005581\n",
      "  also: 0.001526\n",
      "  take: 0.001303\n",
      "\n",
      " 2.D\n",
      "Generated sentence: market is the first time he said it has a\n",
      "\n",
      " 2.E\n",
      "Generated sentence (With Smoothing Implementation): market and pct from the first quarter the us agriculture\n",
      "\n",
      " 3.A\n",
      "\n",
      "--- Testing Text Generation with Various Prefixes for Different n-gram Sizes ---\n",
      "\n",
      "--- 2-gram Model Text Generation ---\n",
      "Prefix ['the']: the company said the first quarter the company\n",
      "Prefix ['in']: in the us agriculture department said it is\n",
      "Prefix ['it']: it is expected to a year the company\n",
      "Prefix ['they']: they said the us and mln dlrs in\n",
      "Prefix ['she']: she said the us agriculture committee of mln\n",
      "\n",
      "--- 3-gram Model Text Generation ---\n",
      "Prefix ['the', 'market']: the market will expand the budget deficit and\n",
      "Prefix ['in', 'the']: in the past two weeks ago but it\n",
      "Prefix ['it', 'is']: it is the worlds biggest institutional strugglesvbz strugglesvbz\n",
      "Prefix ['they', 'were']: they were not enough to ensure that the\n",
      "Prefix ['she', 'was']: she was a lot girl on cell yeah\n",
      "\n",
      "--- 4-gram Model Text Generation ---\n",
      "Prefix ['the', 'stock', 'market']: the stock market overreacted to japan trade ivorynntl\n",
      "Prefix ['in', 'the', 'city']: in the city of oruro millions ivorynntl millions\n",
      "Prefix ['it', 'is', 'a']: it is a major development for ivorynntl strugglesvbz\n",
      "Prefix ['they', 'were', 'not']: they were not convinced ivorynntl wardennn glottochronologynn glottochronologynn\n",
      "Prefix ['she', 'was', 'very']: she was very strugglesvbz wardennn millions strugglesvbz millions\n",
      "\n",
      "--- 5-gram Model Text Generation ---\n",
      "Prefix ['the', 'stock', 'market', 'is']: the stock market is strugglesvbz millions ivorynntl millions\n",
      "Prefix ['in', 'the', 'middle', 'of']: in the middle of the street it millions\n",
      "Prefix ['it', 'is', 'a', 'good']: it is a good millions wardennn wardennn strugglesvbz\n",
      "Prefix ['they', 'were', 'not', 'able']: they were not able strugglesvbz wardennn ivorynntl glottochronologynn\n",
      "Prefix ['she', 'was', 'very', 'happy']: she was very happy strugglesvbz wardennn glottochronologynn glottochronologynn\n",
      "\n",
      " 3.B. Perplexity Evaluation\n",
      "Perplexity for n=2: 62.8202\n",
      "\n",
      "Perplexity for n-grams from 2 to 5:\n",
      "Perplexity for n=2: 85.0928\n",
      "Perplexity for n=3: 36.1999\n",
      "Perplexity for n=4: 36.5359\n",
      "Perplexity for n=5: 40.5457\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Prepare the training data\n",
    "    raw_text = get_training_data()\n",
    "    cleaned_text = preprocess_text(raw_text)\n",
    "    print(f\"\\nSample of cleaned text: {cleaned_text[:500]}\")\n",
    "    tokens = tokenize_text(cleaned_text)\n",
    "    print(f\"Total tokens: {len(tokens)}\")\n",
    "    train_tokens, test_tokens = split_data(tokens, train_ratio=0.8)\n",
    "    print(f\"Training tokens: {len(train_tokens)}\")\n",
    "    print(f\"Testing tokens: {len(test_tokens)}\")\n",
    "    vocabulary = get_vocabulary(train_tokens)\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    print(f\"Vocabulary size: {vocabulary_size}\")\n",
    "    \n",
    "    n = 2\n",
    "\n",
    "    # 2.a. Create n-grams from the tokenized text and calculate their frequencies in the dataset.\n",
    "    print(f\"\\n 2.A. Sucessfully created n-grams from the tokenized text and calculated their frequencies in the dataset.\")\n",
    "    ngram_list = create_ngrams(train_tokens, n)\n",
    "    ngram_counts = count_ngrams(ngram_list)\n",
    "    context_list = create_ngrams(train_tokens, n-1)\n",
    "    context_counts = count_ngrams(context_list)\n",
    "\n",
    "    # 2.b. Calculate the probability of a word following a given (n − 1)-gram.\n",
    "    print(f\"\\n 2.B\")\n",
    "    probability = calculate_probability(\n",
    "        word=\"market\",\n",
    "        context=(\"the\",),\n",
    "        ngram_counts=ngram_counts,\n",
    "        context_counts=context_counts,\n",
    "        vocabulary_size=vocabulary_size,\n",
    "        smoothing='laplace'\n",
    "    )\n",
    "    print(f\"Probability of 'market' following 'the': {probability:.6f}\")\n",
    "    \n",
    "    # 2.c. Write a function to predict the next word given a sequence of words based on these probabilities.\n",
    "    print(f\"\\n 2.C\")\n",
    "    predict_next_word_main( vocabulary, ngram_counts, context_counts,n)\n",
    "\n",
    "    # 2.d. Predict the next word given a sequence of words based on these probabilities. \n",
    "    print(f\"\\n 2.D\")\n",
    "    sentence = generate_sentence( length=10, vocabulary=vocabulary, ngram_counts=ngram_counts, context_counts=context_counts, n=n, prefix=[\"the\", \"market\"], smoothing='none')\n",
    "    print(f\"Generated sentence: {' '.join(sentence)}\")\n",
    "\n",
    "    # 2.e. Implement smoothing techniques (like Laplace smoothing) to handle the issues of zero probabilities for unseen n-grams.\n",
    "    print(f\"\\n 2.E\")\n",
    "    sentence = generate_sentence( length=10, vocabulary=vocabulary, ngram_counts=ngram_counts, context_counts=context_counts, n=n, prefix=[\"the\", \"market\"], smoothing='laplace')\n",
    "    print(f\"Generated sentence (With Smoothing Implementation): {' '.join(sentence)}\")\n",
    "\n",
    "\n",
    "    #3.a Test the model by inputting various prefixes (with at most n - 1 words) and evaluating its ability to generate text.\n",
    "    print(f\"\\n 3.A\")\n",
    "    test_nGram_generation(n, train_tokens, vocabulary, smoothing='laplace')\n",
    "\n",
    "    #3.b Compute the perplexity of the model on a test set that was not used during training.\n",
    "    print(\"\\n 3.B. Perplexity Evaluation\")\n",
    "    perplexity = calculate_perplexity(\n",
    "        test_tokens=test_tokens,\n",
    "        ngram_counts=ngram_counts,\n",
    "        context_counts=context_counts,\n",
    "        vocabulary_size=vocabulary_size,\n",
    "        n=n,\n",
    "        smoothing='none'\n",
    "    )\n",
    "    print(f\"Perplexity for n={n}: {perplexity:.4f}\")\n",
    "\n",
    "    #3.c Compare the performance of models with different values of n (e.g., bigrams vs. trigrams vs. 4-grams). \n",
    "    # Discuss which model achieves lower perplexity and provide insights into why certain n values might be more effective in various contexts.\n",
    "    print(\"\\nPerplexity for n-grams from 2 to 5:\")\n",
    "    test_perplexity_for_ngram_range(vocabulary, test_tokens, train_tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbcf19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
