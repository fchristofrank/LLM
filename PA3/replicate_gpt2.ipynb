{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0467a9-41a8-4a22-a2c3-e6af22da9b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61ba16d-ac66-44bd-ae51-f5b522e71a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Part 1: BPE Tokenizer Implementation =============\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"Byte-Pair Encoding tokenizer implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=32000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_tokenize = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        self.encoder = {}\n",
    "        self.decoder = {}\n",
    "        self.bpe_ranks = {}\n",
    "        \n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Count frequency of adjacent pairs\"\"\"\n",
    "        pairs = Counter()\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, v_in):\n",
    "        \"\"\"Merge the most frequent pair\"\"\"\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in v_in:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = v_in[word]\n",
    "        return v_out\n",
    "    \n",
    "    def train(self, texts, min_frequency=2):\n",
    "        \"\"\"Train BPE on texts\"\"\"\n",
    "        # Get word frequencies\n",
    "        word_freqs = Counter()\n",
    "        for text in tqdm(texts, desc=\"Counting words\"):\n",
    "            words = self.word_tokenize.findall(text)\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "        \n",
    "        # Initialize vocabulary with characters\n",
    "        vocab = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            if freq >= min_frequency:\n",
    "                vocab[' '.join(list(word)) + ' </w>'] = freq\n",
    "        \n",
    "        # Build initial vocabulary\n",
    "        alphabet = []\n",
    "        for word in vocab.keys():\n",
    "            for symbol in word.split():\n",
    "                if symbol not in alphabet:\n",
    "                    alphabet.append(symbol)\n",
    "        \n",
    "        # Start with alphabet tokens\n",
    "        for i, symbol in enumerate(alphabet):\n",
    "            self.encoder[symbol] = i\n",
    "            self.decoder[i] = symbol\n",
    "        \n",
    "        # Learn merges\n",
    "        num_merges = self.vocab_size - len(alphabet)\n",
    "        for i in tqdm(range(num_merges), desc=\"Learning BPE\"):\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            \n",
    "            # Add new token\n",
    "            token = ''.join(best)\n",
    "            token_id = len(self.encoder)\n",
    "            self.encoder[token] = token_id\n",
    "            self.decoder[token_id] = token\n",
    "            self.bpe_ranks[best] = i\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text using learned BPE\"\"\"\n",
    "        tokens = []\n",
    "        words = self.word_tokenize.findall(text)\n",
    "        \n",
    "        for word in words:\n",
    "            word = ' '.join(list(word)) + ' </w>'\n",
    "            \n",
    "            if word in self.encoder:\n",
    "                tokens.append(self.encoder[word])\n",
    "                continue\n",
    "                \n",
    "            while True:\n",
    "                pairs = []\n",
    "                symbols = word.split()\n",
    "                for i in range(len(symbols) - 1):\n",
    "                    pair = (symbols[i], symbols[i + 1])\n",
    "                    if pair in self.bpe_ranks:\n",
    "                        pairs.append((self.bpe_ranks[pair], i, pair))\n",
    "                \n",
    "                if not pairs:\n",
    "                    break\n",
    "                    \n",
    "                _, i, pair = min(pairs)\n",
    "                symbols[i] = ''.join(pair)\n",
    "                symbols.pop(i + 1)\n",
    "                word = ' '.join(symbols)\n",
    "            \n",
    "            for symbol in symbols:\n",
    "                if symbol in self.encoder:\n",
    "                    tokens.append(self.encoder[symbol])\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Decode tokens back to text\"\"\"\n",
    "        text = ''.join([self.decoder.get(token, '') for token in tokens])\n",
    "        text = text.replace('</w>', ' ')\n",
    "        return text.strip()\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save tokenizer to file\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'encoder': self.encoder,\n",
    "                'decoder': {int(k): v for k, v in self.decoder.items()},\n",
    "                'bpe_ranks': {f\"{k[0]} {k[1]}\": v for k, v in self.bpe_ranks.items()},\n",
    "                'vocab_size': self.vocab_size\n",
    "            }, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load tokenizer from file\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.encoder = data['encoder']\n",
    "        self.decoder = {int(k): v for k, v in data['decoder'].items()}\n",
    "        self.bpe_ranks = {tuple(k.split()): v for k, v in data['bpe_ranks'].items()}\n",
    "        self.vocab_size = data['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa3174d6-7a9c-4938-a290-889b07f33e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Part 2: GPT-2 Model Architecture =============\n",
    "\n",
    "class GPT2Config:\n",
    "    \"\"\"Configuration for GPT-2 model\"\"\"\n",
    "    def __init__(self, model_size='small'):\n",
    "        configs = {\n",
    "            'small': {'n_layer': 12, 'n_head': 12, 'n_embd': 768},\n",
    "            'medium': {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
    "            'large': {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
    "            'xl': {'n_layer': 48, 'n_head': 25, 'n_embd': 1600}\n",
    "        }\n",
    "        \n",
    "        config = configs[model_size]\n",
    "        self.n_layer = config['n_layer']\n",
    "        self.n_head = config['n_head']\n",
    "        self.n_embd = config['n_embd']\n",
    "        self.vocab_size = 32000\n",
    "        self.block_size = 128\n",
    "        self.dropout = 0.1\n",
    "\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    \"\"\"Transformer block with pre-layer norm\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            config.n_embd, \n",
    "            config.n_head, \n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm attention with residual\n",
    "        attn_out, _ = self.attn(self.ln_1(x), self.ln_1(x), self.ln_1(x), attn_mask=mask)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # Pre-norm MLP with residual\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2Model(nn.Module):\n",
    "    \"\"\"GPT-2 model implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([GPT2Block(config) for _ in range(config.n_layer)])\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Sequence length {t} exceeds block size {self.config.block_size}\"\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        tok_emb = self.wte(idx)\n",
    "        pos_emb = self.wpe(pos)\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = torch.triu(torch.ones(t, t, device=idx.device) * float('-inf'), diagonal=1)\n",
    "        \n",
    "        # Forward through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # Calculate loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, strategy='greedy', top_k=50, top_p=0.9):\n",
    "        \"\"\"Generate text using different sampling strategies\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if needed\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply sampling strategy\n",
    "            if strategy == 'greedy':\n",
    "                idx_next = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "            elif strategy == 'top_k':\n",
    "                # Top-k sampling\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "                probs = F.softmax(top_k_logits, dim=-1)\n",
    "                idx_next = torch.gather(top_k_indices, -1, torch.multinomial(probs, 1))\n",
    "            elif strategy == 'nucleus':\n",
    "                # Nucleus (top-p) sampling\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                \n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, 1)\n",
    "            \n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6723a6-b4f1-4c80-af9e-f050c54f0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Part 3: Dataset and Training =============\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for language modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Tokenize all texts\n",
    "        self.tokens = []\n",
    "        for text in tqdm(texts, desc=\"Tokenizing dataset\"):\n",
    "            self.tokens.extend(tokenizer.tokenize(text))\n",
    "        \n",
    "        print(f\"Total tokens: {len(self.tokens)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def create_dataloaders(train_texts, val_texts, tokenizer, block_size, batch_size):\n",
    "    \"\"\"Create train and validation dataloaders\"\"\"\n",
    "    train_dataset = TextDataset(train_texts, tokenizer, block_size)\n",
    "    val_dataset = TextDataset(val_texts, tokenizer, block_size)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "class CosineWarmupScheduler:\n",
    "    \"\"\"Learning rate scheduler with warmup and cosine decay\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=1e-5):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.base_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "        \n",
    "    def step(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            factor = step / self.warmup_steps\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            progress = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            factor = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        for i, group in enumerate(self.optimizer.param_groups):\n",
    "            group['lr'] = self.min_lr + (self.base_lrs[i] - self.min_lr) * factor\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, device, epochs=10, \n",
    "                learning_rate=3e-4, weight_decay=0.1, warmup_ratio=0.1,\n",
    "                checkpoint_dir='checkpoints', log_interval=100):\n",
    "    \"\"\"Train the GPT-2 model\"\"\"\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Setup scheduler\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(warmup_ratio * total_steps)\n",
    "    scheduler = CosineWarmupScheduler(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    step = 0\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss_sum = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_idx, (x, y) in enumerate(pbar):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, loss = model(x, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step(step)\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            train_steps += 1\n",
    "            step += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'})\n",
    "            \n",
    "            # Log and save checkpoint\n",
    "            if step % log_interval == 0:\n",
    "                avg_train_loss = train_loss_sum / train_steps\n",
    "                train_losses.append((step, avg_train_loss))\n",
    "                \n",
    "                # Validation\n",
    "                model.eval()\n",
    "                val_loss_sum = 0\n",
    "                val_steps = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for x_val, y_val in val_loader:\n",
    "                        x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                        _, loss = model(x_val, y_val)\n",
    "                        val_loss_sum += loss.item()\n",
    "                        val_steps += 1\n",
    "                        \n",
    "                        if val_steps >= 50:  # Evaluate on subset\n",
    "                            break\n",
    "                \n",
    "                avg_val_loss = val_loss_sum / val_steps\n",
    "                val_losses.append((step, avg_val_loss))\n",
    "                \n",
    "                print(f\"\\nStep {step}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                checkpoint = {\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'config': config\n",
    "                }\n",
    "                torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_step_{step}.pt'))\n",
    "                \n",
    "                model.train()\n",
    "                train_loss_sum = 0\n",
    "                train_steps = 0\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate_perplexity(model, test_loader, device):\n",
    "    \"\"\"Evaluate model perplexity on test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(test_loader, desc=\"Evaluating perplexity\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, y)\n",
    "            \n",
    "            total_loss += loss.item() * x.size(0) * x.size(1)\n",
    "            total_tokens += x.size(0) * x.size(1)\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a274f25-65f1-48b5-b825-fa5caa58c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Part 4: Main Training Script =============\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load dataset (using WikiText-2 as example)\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "    \n",
    "    # Split data\n",
    "    train_texts = dataset['train']['text'][:10000]  # Limit for demo\n",
    "    val_texts = dataset['validation']['text'][:1000]\n",
    "    test_texts = dataset['test']['text'][:1000]\n",
    "    \n",
    "    # Filter out empty texts\n",
    "    train_texts = [t for t in train_texts if t.strip()]\n",
    "    val_texts = [t for t in val_texts if t.strip()]\n",
    "    test_texts = [t for t in test_texts if t.strip()]\n",
    "    \n",
    "    # Initialize and train tokenizer\n",
    "    print(\"Training tokenizer...\")\n",
    "    tokenizer = BPETokenizer(vocab_size=32000)\n",
    "    tokenizer.train(train_texts[:5000])  # Train on subset\n",
    "    tokenizer.save('tokenizer.json')\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    config = GPT2Config(model_size='small')\n",
    "    model = GPT2Model(config).to(device)\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    print(\"Creating dataloaders...\")\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        train_texts, val_texts, tokenizer, \n",
    "        block_size=config.block_size, batch_size=8\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader, config, device,\n",
    "        epochs=3, learning_rate=3e-4, log_interval=100\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    train_steps, train_loss_values = zip(*train_losses)\n",
    "    val_steps, val_loss_values = zip(*val_losses)\n",
    "    plt.plot(train_steps, train_loss_values, label='Train Loss')\n",
    "    plt.plot(val_steps, val_loss_values, label='Validation Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_loader = DataLoader(\n",
    "        TextDataset(test_texts, tokenizer, config.block_size),\n",
    "        batch_size=8, shuffle=False\n",
    "    )\n",
    "    perplexity, test_loss = evaluate_perplexity(model, test_loader, device)\n",
    "    print(f\"Test Perplexity: {perplexity:.2f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Generate text samples\n",
    "    print(\"\\nGenerating text samples...\")\n",
    "    model.eval()\n",
    "    \n",
    "    prompts = [\n",
    "        \"The weather today is\",\n",
    "        \"In the beginning of time\",\n",
    "        \"Machine learning is\"\n",
    "    ]\n",
    "    \n",
    "    strategies = ['greedy', 'top_k', 'nucleus']\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        tokens = tokenizer.tokenize(prompt)\n",
    "        x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(x, max_new_tokens=50, strategy=strategy)\n",
    "                text = tokenizer.decode(generated[0].tolist())\n",
    "                print(f\"\\n{strategy.capitalize()} sampling: {text}\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'gpt2_final.pt')\n",
    "    \n",
    "    # Create report\n",
    "    report = f\"\"\"\n",
    "# GPT-2 Training Report\n",
    "\n",
    "## Model Configuration\n",
    "- Model Size: Small\n",
    "- Parameters: {total_params:,}\n",
    "- Layers: {config.n_layer}\n",
    "- Heads: {config.n_head}\n",
    "- Embedding Dimension: {config.n_embd}\n",
    "- Sequence Length: {config.block_size}\n",
    "- Vocabulary Size: {config.vocab_size}\n",
    "\n",
    "## Training Results\n",
    "- Final Training Loss: {train_loss_values[-1]:.4f}\n",
    "- Final Validation Loss: {val_loss_values[-1]:.4f}\n",
    "- Test Perplexity: {perplexity:.2f}\n",
    "- Test Loss: {test_loss:.4f}\n",
    "\n",
    "## Text Generation Examples\n",
    "\n",
    "### Prompt: \"The weather today is\"\n",
    "- Greedy: [Generated text would appear here]\n",
    "- Top-k: [Generated text would appear here]\n",
    "- Nucleus: [Generated text would appear here]\n",
    "\n",
    "### Analysis\n",
    "The model successfully learned to generate coherent text. Different sampling strategies produced varying levels of creativity and coherence:\n",
    "- Greedy decoding produced the most predictable but coherent text\n",
    "- Top-k sampling added more diversity while maintaining quality\n",
    "- Nucleus sampling provided the best balance between creativity and coherence\n",
    "\n",
    "## Training Curves\n",
    "See training_curves.png for loss visualization.\n",
    "\"\"\"\n",
    "    \n",
    "    with open('training_report.md', 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"\\nTraining complete! Check training_report.md for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95bfbec7-37a0-4b82-8145-f7cf438affee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\frank\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating test split: 100%|████████████████████████████████████████████| 4358/4358 [00:00<00:00, 484167.53 examples/s]\n",
      "Generating train split: 100%|████████████████████████████████████████| 36718/36718 [00:00<00:00, 1311329.36 examples/s]\n",
      "Generating validation split: 100%|██████████████████████████████████████| 3760/3760 [00:00<00:00, 626264.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words: 100%|███████████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 19305.05it/s]\n",
      "Learning BPE:  85%|████████████████████████████████████████████████████▋         | 27022/31811 [07:24<01:18, 60.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Total parameters: 134,306,304\n",
      "Creating dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|████████████████████████████████████████████████████████| 6520/6520 [00:03<00:00, 2155.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 613594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████████████████████████████████████████████████████| 644/644 [00:00<00:00, 2586.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 56053\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|                                      | 51/76684 [06:15<156:43:08,  7.36s/it, loss=8.8837, lr=1.06e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[0;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Plot training curves\u001b[39;00m\n\u001b[0;32m     52\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[1;32mIn[7], line 97\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, config, device, epochs, learning_rate, weight_decay, warmup_ratio, checkpoint_dir, log_interval)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 97\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d1e959-eef3-4644-904f-c761ee4f5ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a9aee-7e45-423f-bd6f-f307a7774e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
