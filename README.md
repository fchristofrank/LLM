Simple Bigram Language Model in PyTorch
A minimal implementation of a bigram language model that predicts the next token based solely on the current token. Features a single embedding layer that serves as a lookup table for next-token probabilities. Includes training with cross-entropy loss and autoregressive text generation. Perfect for understanding the basics of language modeling before diving into transformers.
Built with PyTorch | Based on Karpathy's "Let's build GPT" tutorial
