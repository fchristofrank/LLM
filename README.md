ğŸ§  Simple Bigram Language Model in PyTorch
A minimal implementation of a Bigram Language Model using PyTorch.

This project demonstrates the core principles of language modeling by predicting the next token based solely on the current token. It serves as an ideal starting point for understanding autoregressive language models before diving into more advanced architectures like Transformers.

Inspired by Andrej Karpathyâ€™s â€œLetâ€™s build GPTâ€ tutorial.

ğŸ“š Overview
This Bigram Language Model:

Uses a single nn.Embedding layer as a lookup table for next-token prediction.

Trains on character-level inputs using cross-entropy loss.

Generates new text autoregressively by sampling one token at a time.

Is simple, yet effective, for educational purposes.

ğŸš€ Features
ğŸ”¢ Token-level embedding for input-output mapping

ğŸ” Autoregressive sampling for text generation

ğŸ§ª Minimal training loop with torch.nn and torch.optim

ğŸ§¼ Clean, readable, and beginner-friendly code

ğŸ“¦ No dependencies beyond PyTorch

ğŸ› ï¸ Installation
bash
Copy
Edit
git clone https://github.com/your-username/simple-bigram-model
cd simple-bigram-model
pip install torch
ğŸ“„ Usage
1. Prepare Input
Put a plain text file named input.txt in the repo directory.

Example:

bash
Copy
Edit
echo "hello world" > input.txt
2. Train the Model
bash
Copy
Edit
python bigram.py
3. Generate Text
The model will print generated samples during and after training.

ğŸ“ File Structure
graphql
Copy
Edit
ğŸ“¦simple-bigram-model
 â”£ ğŸ“„input.txt           # Training data (plain text)
 â”£ ğŸ“„bigram.py           # Main training & generation script
 â”— ğŸ“„README.md           # Project documentation
ğŸ” Example Output
yaml
Copy
Edit
Generated sample:
tht tnd t tnd tot tnnt nttddhdnt hthdttdt...
(Not coherent yetâ€”but thatâ€™s the beauty of starting simple!)

ğŸ§  Learn More
This repo is educational in nature and best understood alongside the original tutorial:

ğŸ“º Karpathyâ€™s â€œLetâ€™s build GPTâ€

ğŸ¤ Contributing
Contributions, ideas, and improvements are welcome! Feel free to open an issue or submit a PR.

ğŸ“œ License
MIT License

